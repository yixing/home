## 关于【浏览器输入网址之后发生了什么】

也许很多人在面试中遇到过这个问题，或者曾经把它当做面试题来让候选人解答。这个问题的背后其实是众多web开发者和服务端研发们不断努力优化的过程。今天我们大致梳理下这个问题的答案。

对于一个典型的HTTP请求来说，从客户端发起到请求完成，经历的延迟环节主要如下：

- DNS解析：客户端在发起请求前需要获得客户端的IP地址，主要经过 客户端 > Local DNS（ > 权威DNS）
- TCP握手：在第一次连接服务端时，客户端发起三次握手，一般为1RTT延迟
- SSL握手：协商秘钥和加密算法，一般为1-2RTT
- 请求发送：客户端在完成握手之后发送请求数据。主要是客户端延迟和TCP传输延迟。
- 服务端处理：服务端收到请求之后对数据进行解析和处理，生成响应数据并发送到TCP。
- 接收响应：响应数据通过TCP传输到客户端

![image-20190325005102621](/Users/yixing/dev/yixing.github.io/img/http-water-fall.png)

## DNS解析

以www.baidu.com为例，浏览器首先要做的事情就是解析出www.baidu.com的地址。对于像浏览器这种复杂的程序来说，进程内的缓存是必不可少的。当一个程序自己的缓存没有的情况下，就会发起DNS查询。

在linux操作系统中，发起DNS查询一般是通过gethostbyname()这种调用，它到底做了哪些事情？可以通过strace下面的代码试试

```c++
#include <netdb.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <iostream>
 
int main(int argc, char*argv[])
{
    struct hostent *hent = gethostbyname(argv[1]);
    for (int i = 0; hent->h_addr_list[i] ;++i) {
        std::cout << inet_ntoa(*(struct in_addr*)hent->h_addr_list[i]) << std::endl;
    }
}
```

```bash
$strace ./a.out www.baidu.com
```

```
execve("./a.out", ["./a.out", "www.baidu.com"], [/* 22 vars */]) = 0
......
open("/etc/resolv.conf", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=99, ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe030059000
read(3, "# Generated by NetworkManager\nna"..., 4096) = 99
read(3, "", 4096)                       = 0
close(3)                                = 0
......
socket(AF_LOCAL, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 3
connect(3, {sa_family=AF_LOCAL, sun_path="/var/run/nscd/socket"}, 110) = -1 ENOENT (No such file or directory)
close(3)                                = 0
......
open("/etc/hosts", O_RDONLY|O_CLOEXEC)  = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=246, ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe030059000
read(3, "127.0.0.1   localhost localhost."..., 4096) = 246
read(3, "", 4096)                       = 0
close(3)                                = 0
......
socket(AF_INET, SOCK_DGRAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 3
connect(3, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr("172.16.0.2")}, 16) = 0
poll([{fd=3, events=POLLOUT}], 1, 0)    = 1 ([{fd=3, revents=POLLOUT}])
sendto(3, "\214`\1\0\0\1\0\0\0\0\0\0\3www\5baidu\3com\0\0\1\0\1", 31, MSG_NOSIGNAL, NULL, 0) = 31
poll([{fd=3, events=POLLIN}], 1, 1000)  = 1 ([{fd=3, revents=POLLIN}])
ioctl(3, FIONREAD, [93])                = 0
recvfrom(3, "\214`\201\200\0\1\0\3\0\0\0\0\3www\5baidu\3com\0\0\1\0\1\300"..., 1024, 0, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr("172.16.0.2")}, [16]) = 93
close(3)                                = 0
......
write(1, "14.215.177.39\n", 1414.215.177.39)         = 14
write(1, "14.215.177.38\n", 1414.215.177.38)         = 14
exit_group(0)     = ?
+++ exited with 0 +++

```

从上面strace的执行记录可以看到，系统自带的DNS查询做了以下几个事情

- 查询/etc/resolve.conf，获取DNS 解析配置和resover地址
- 尝试查询ncsd服务，这是一个linux系统的本地缓存服务
- 查询/etc/hosts，寻找本地配置记录
- 创建UDP socket，向Local DNS发起查询，解析结果

下面，看看Local DNS服务器是怎样处理的这次查询，我们配置一台支持递归查询的BIND，为了避免有额外的抓包数据，关闭了DNSSEC功能：
```
# /etc/named.conf
options {
	listen-on port 53 { 127.0.0.1; };
	directory 	"/var/named";
	dump-file 	"/var/named/data/cache_dump.db";
	statistics-file "/var/named/data/named_stats.txt";
	memstatistics-file "/var/named/data/named_mem_stats.txt";
	recursing-file  "/var/named/data/named.recursing";
	secroots-file   "/var/named/data/named.secroots";
	allow-query     { localhost; };

	recursion yes;

	dnssec-enable no;
	dnssec-validation no;

	bindkeys-file "/etc/named.iscdlv.key";
	managed-keys-directory "/var/named/dynamic";

	pid-file "/run/named/named.pid";
	session-keyfile "/run/named/session.key";
};

logging {
        channel default_debug {
                file "data/named.run";
                severity dynamic;
        };
};

zone "." IN {
	type hint;
	file "named.ca";
};

include "/etc/named.rfc1912.zones";
include "/etc/named.root.key";
```
named.ca采用默认的即可，也可以下载最新的：
```bash
wget https://www.internic.net/domain/named.root -O /var/named/named.ca
```
启动后，启动tcpdump抓包，然后向本机发起DNS查询请求：
```bash
/usr/sbin/tcpdump -w dump.pcap -i any port 53 &
dig @127.0.0.1 www.baidu.com
```
响应返回后，抓取到的数据如下：

![image-20190323234700062](/Users/yixing/dev/yixing.github.io/img/image-20190323234700062.png)
以上16个报文的含义

1. dig命令触发向本机udp 53端口发起www.baidu.com的DNS查询请求
2. BIND向一台根服务器发起www.baidu.com的DNS查询请求
3. BIND向根服务器发起更新根列表的请求(与本次功能无关)
4. 根服务器返回.com域名的权威NS列表，包含一系列的gtld-servers.net服务器地址
5. BIND选取一台.com的权威NS，发起www.baidu.com的DNS查询请求
6. 根服务器对请求3返回的根服务器列表(与本功能无关)
7. .com权威NS返回baidu.com的权威NS列表，包含一系列nsx.baidu.com服务器地址
8. BIND选取一台baidu.com的权威NS，发起www.baidu.com的DNS查询请求
9. 权威NS返回www.baidu.com的解析结果，表示www.baidu.com应该别名映射到www.a.shifen.com，CNAME列表，同时给出了a.shifen.com的权威NS列表
10. 由于a.shifen.com本身不和baidu.com在同一个域，所以BIND重新向一台.com的权威NS发起了www.a.shifen.com的DNS查询
11. .com权威返回shifen.com的权威NS列表
12. BIND向一台shifen.com的权威NS发起www.a.shifen.com的DNS查询
13. shifen.com的权威NS返回a.shifen.com的权威NS列表
14. BIND向一台a.shifen.com的权威NS发起www.a.shifen.com的DNS查询
15. a.shifen.com的权威NS返回www.a.shifen.com的A记录
16. BIND将A记录返回给客户端，至此dig命令获取到最终的IP地址记录

![image-20190325011016022](/Users/yixing/dev/yixing.github.io/img/www-baidu-com-dns.png)

从以上记录中可以看到，查询从根服务开始，经过.com > baidu.com > .com >  shifen.com > a.shifen.com，到最终www.a.shifen.com被解析，一共6次请求往返，BIND才获取到最终结果，Local DNS所做的事情也是如此。只是并不是每次查询都需要经过这个漫长的过程，因为之前的结果在指定的TTL之内是可缓存的。例如之后发起第二次dig查询的时候，tcpdump看到的结果如下

```
00:12:22.303060 IP 127.0.0.1.42743 > 127.0.0.1.53: 5883+ [1au] A? www.baidu.com. (42)
00:12:22.303348 IP 127.0.0.1.53 > 127.0.0.1.42743: 5883 3/5/6 CNAME www.a.shifen.com., A 220.181.111.37, A 220.181.112.244 (271)
```

可见BIND没有向外发起任何查询，直接返回了结果，因此当一个域名访问量很大的时候，大概率是能够从Local DNS直接拿到IP地址的。

以上就是客户端拿到IP地址所需要经历的过程，下一步就是发起TCP连接，发送请求了。

## TCP握手

TCP传输的起点，就是我们熟悉的三次握手过程

![Figure 2-1. Three-way handshake](/Users/yixing/dev/yixing.github.io/img/TCP-handshak.svg)

在客户端发送最后一个ACK的同时，就可以发送实际的请求数据，因此握手的开销就是1个RTT。不过后来Google提出了一种优化的方案[TCP Fast Open, RFC7413](https://tools.ietf.org/html/rfc7413)，来实现0-RTT握手，只是并没有得到广泛的应用。

握手完成之后，TCP的传输就可以真正开始了，进入真正的请求发送阶段，对于客户端来说，确定一个请求的发送时间还有两个变数：

1. HTTPS请求的TLS握手
2. TCP的拥塞控制

## TLS 握手

TLS握手发生在TCP握手完成之后，最坏的情况下，客户端和服务端进行一次完整的TLS握手需要两次RTT:
```
       Client                                                      Server
  
       ClientHello                  --------> 
                                                              ServerHello
                                                             Certificate*
                                                       ServerKeyExchange*
                                                      CertificateRequest*
                                    <--------             ServerHelloDone
       Certificate*
       ClientKeyExchange 
       CertificateVerify* 
       [ChangeCipherSpec] 
       Finished                     --------> 
                                                       [ChangeCipherSpec]
                                    <--------                    Finished
       Application Data             <------->            Application Data
 
              Figure 1.  Message flow for a full handshake
```

TLS采用了多种优化措施来降低这种开销，包括原本设计中的session resumption，和后续的改进：[Session Resumption without Server-Side State, RFC5077](https://tools.ietf.org/html/rfc5077)，[TLS False Start, RFC7918](https://tools.ietf.org/html/rfc7918)，以及[TLS 1.3, RFC8446](https://tools.ietf.org/html/rfc8446)，这些都实现了1-RTT完成握手，甚至[0-RTT完成握手](https://tools.ietf.org/html/rfc8446#section-2.3)。

## 请求发送

TLS握手完成后，就开始实际的请求数据传输，对于明文的HTTP请求来说，请求的size就是实际的TCP payload size。如果是HTTPS的请求就是加密后的数据的实际长度。

我们知道在7层网络模型中，TCP数据需要被封装为网络层的IP报文，再封装为链路层的数据帧。由于[以太网的帧长度一般限制为1518](https://howdoesinternetwork.com/2018/mtu)，因此留给IP报文的最大长度（Maximum Transmission Unix, MTU）一般是1500字节。

![Ethernet_Type_II_Frame_format](/Users/yixing/dev/yixing.github.io/img/Ethernet_Type_II_Frame_format.svg)

在MTU的基础上，减去20字节的IP Header和20字节的TCP header，可知在正常情况下，TCP层交付给IP层的最大长度（Maximum Segment Size, MSS）就是1460字节。

如果不考虑服务端TCP的接收速度，即无论客户端发送多快服务端都可以接收（一般情况下确实如此），那么只需要考虑客户端的发送速度就行了，那么问题就变成了，多久才能发送完 size/1460 个数据包？回答这个问题就需要知道TCP数据是怎样发送的了。

首先的问题是一开始可以发几个数据包，这个在操作系统中一般是个固定值，称为初始窗口(Initial Window，IW)。早起的TCP实现初始窗口是一般就是1个报文，即一开始只发一个1460字节的TCP数据包，在这个收到对端返回的ACK之后才会发送后续的数据。每收到一个ACK的时候窗口size+1，逐渐增大拥塞窗口，这样一来每次RTT窗口size翻倍，直至达到上限（slow start threshold，ssthresh），这个过程就是TCP的慢启动。

![image-20190325004639321](/Users/yixing/dev/yixing.github.io/img/tcp-slow-start.png)

在 [RFC 2581(1999)](https://tools.ietf.org/html/rfc2581) 中，TCP的初始窗口明确规定为小于等于2，后来经过数次调整，最终在[RFC 6928(2013)](https://tools.ietf.org/html/rfc6928) 中将初始窗口修改为10。linux 3.0版本之后的内核中初始窗口便采用了这个值。

在不考虑丢包和上限的情况下，TCP的拥塞窗口变化趋势就是一个指数递增的过程，从IW开始，每个RTT增大一倍，最终发送size大小的数据所需要的时间与RTT直接相关：

![image-20190325192722843](/Users/yixing/dev/yixing.github.io/img/TCP-size-time.png)


这个公式大致能计算出一个请求的发送时间。

## 响应接收
以上，就是请求发送完成所需要的各个环节。有了上面的理解，服务器处理完请求，将响应发送回来所需要的时间就比较简单了，基本上就是传输响应体数据所需要的时间。与客户端发送的过程一样，TCP的发送一样要通过慢启动的过程。

## 优化

现在我们已经基本弄清楚了一个HTTP请求的各个阶段耗时，那么对于开发者来说，最关心的就是怎样优化这些环节。如果客户端是一个APP，那么有很多办法可以DNS解析和连接握手的开销，比如：

- 主动DNS预热，异步DNS更新
在应用启动之后直接发起一次DNS查询，之后的请求直接使用DNS的查询结果即可。通过异步的线程继续定期的更新DNS结果，那么就解决了DNS耗时的问题。

- 主动连接池，长连接保持
与DNS预热的原理类似，如果TCP连接在使用前能够建立，那么就降低了握手的开销，也能消除TLS握手带来的影响，如果有提前建立好的连接，那么在使用的时候直接发送数据即可。

这些优化的思路其实都是为了降低不必要的连接状态初始化开销，在连接建立之后，实际的延迟就只跟网络质量相关了。提升网络质量最直接的手段就是提高服务端的网络接入质量，做到更低的RTT和丢包率。由于国内的互联网络主要由多个运营商承载，如果客户端和服务端所在的ISP不同，比如服务端在电信客户端在移动，就不可避免的会有跨网的访问，这会带来明显的访问延迟。因此国内的大网站一般都部署在多线机房，保证不同运营商访问服务端的网络质量，但是付出的成本也是非常可观的。

另一个解决方案就是CDN。一般我们看到的页面上的图片、视频、js、css等静态内容，会通过CDN缓存到全国各地的服务器中，CDN的调度系统会将用户的请求指向离用户最近的节点上，一般是本省内的节点，并且最好与客户端处于同一个网络运营商。这样一来客户端的访问延迟就能降低到一个比较低的水平。

那么动态请求呢？比如我们的搜索查询结果，这些都是不可缓存的，对于这个问题，也可以通过CDN的动态加速来解决。

在百度的内部业务测试中，即使是多线机房的源站，也能看到明显的动态加速收益。

![image-20190325005843451](/Users/yixing/dev/yixing.github.io/img/dsa-searcher.png)







